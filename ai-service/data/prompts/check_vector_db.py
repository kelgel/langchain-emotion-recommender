# check_vector_db.py - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ë„êµ¬
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os
from dotenv import load_dotenv

load_dotenv()

def check_vector_database():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    # ChromaDB ê²½ë¡œ ì„¤ì • - í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.dirname(current_dir)  # promptsì˜ ë¶€ëª¨ = data
    persist_directory = os.path.join(data_dir, "chroma_db")

    print("ğŸ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
    print(f"ì €ì¥ ìœ„ì¹˜: {os.path.abspath(persist_directory)}")

    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not os.path.exists(persist_directory):
        print("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    # íŒŒì¼ ëª©ë¡ í™•ì¸
    files = []
    for root, dirs, filenames in os.walk(persist_directory):
        for filename in filenames:
            file_path = os.path.join(root, filename)
            file_size = os.path.getsize(file_path)
            files.append((file_path, file_size))

    print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
    total_size = 0
    for file_path, size in files:
        size_mb = size / (1024 * 1024)
        total_size += size
        print(f"  {file_path}: {size_mb:.2f} MB")

    print(f"\nğŸ’¾ ì´ í¬ê¸°: {total_size / (1024 * 1024):.2f} MB")

    try:
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë° ë¬¸ì„œ ìˆ˜ í™•ì¸
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        vectorstore = Chroma(
            collection_name="bookstore_collection",
            embedding_function=embeddings,
            persist_directory=persist_directory
        )

        # ìƒ˜í”Œ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ ìˆ˜ ì¶”ì •
        test_results = vectorstore.similarity_search("test", k=1)

        if test_results:
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                # Chroma ë‚´ë¶€ API ì‚¬ìš© (ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                collection = vectorstore._collection
                doc_count = collection.count()
                print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")
            except:
                print("ğŸ“Š ë¬¸ì„œ ìˆ˜ëŠ” ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

def sample_search():
    """ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    # ChromaDB ê²½ë¡œ ì„¤ì • - í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.dirname(current_dir)  # promptsì˜ ë¶€ëª¨ = data
    persist_directory = os.path.join(data_dir, "chroma_db")

    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        vectorstore = Chroma(
            collection_name="bookstore_collection",
            embedding_function=embeddings,
            persist_directory=persist_directory
        )

        # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ í…ŒìŠ¤íŠ¸
        test_queries = ["í”„ë¡œê·¸ë˜ë°", "ìë°”", "í–‰ë³µ", "ìŠ¤íŠ¸ë ˆìŠ¤"]

        for query in test_queries:
            results = vectorstore.similarity_search(query, k=3)
            print(f"\n'{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")

            for i, doc in enumerate(results[:2], 1):  # ìƒìœ„ 2ê°œë§Œ í‘œì‹œ
                metadata = doc.metadata
                print(f"\n  {i}. ìƒí’ˆ ì •ë³´:")

                # ê¸°ë³¸ ìƒí’ˆ ì •ë³´
                print(f"     ğŸ“‹ íƒ€ì…: {metadata.get('type', 'unknown')}")
                print(f"     ğŸ“š ISBN: {metadata.get('isbn', 'N/A')}")
                top_cat = metadata.get('top_category_name', '')
                mid_cat = metadata.get('mid_category_name', '')
                low_cat = metadata.get('low_category_name', '')
                if top_cat or mid_cat or low_cat:
                    category_path = ' > '.join(filter(None, [top_cat, mid_cat, low_cat]))
                    print(f"     ğŸ—‚ï¸ ì¹´í…Œê³ ë¦¬: {category_path}")
                print(f"     ğŸ“– ì œëª©: {metadata.get('product_name', 'N/A')}")
                print(f"     âœï¸ ì‘ê°€: {metadata.get('author', 'N/A')}")
                print(f"     ğŸ¢ ì¶œíŒì‚¬: {metadata.get('publisher', 'N/A')}")
                print(f"     ğŸ’° ê°€ê²©: {metadata.get('price', 'N/A')}ì›")
                print(f"     â­ í‰ì : {metadata.get('rate', 'N/A')}")

                # í‚¤ì›Œë“œ ì •ë³´ (JSON íŒŒì‹± í•„ìš”)
                if metadata.get('product_keywords'):
                    try:
                        import json
                        kw = json.loads(metadata['product_keywords'])
                        if isinstance(kw, list):
                            print(f"     ğŸ”‘ ì œí’ˆí‚¤ì›Œë“œ: {', '.join(kw)}")
                        else:
                            print(f"     ğŸ”‘ ì œí’ˆí‚¤ì›Œë“œ: {kw}")
                    except:
                        print(f"     ğŸ”‘ ì œí’ˆí‚¤ì›Œë“œ: {metadata['product_keywords'][:50]}...")

                if metadata.get('product_emotion_keywords'):
                    try:
                        import json
                        kw = json.loads(metadata['product_emotion_keywords'])
                        if isinstance(kw, list):
                            print(f"     ğŸ˜Š ìƒí’ˆê°ì •: {', '.join(kw)}")
                        else:
                            print(f"     ğŸ˜Š ìƒí’ˆê°ì •: {kw}")
                    except:
                        print(f"     ğŸ˜Š ìƒí’ˆê°ì •: {metadata['product_emotion_keywords'][:50]}...")

                # ë¦¬ë·° ì •ë³´ (ì¼ë°˜ í…ìŠ¤íŠ¸ - JSON íŒŒì‹± ë¶ˆí•„ìš”)
                if metadata.get('review_title'):
                    review_title = metadata['review_title']
                    if len(review_title) > 100:
                        review_title = review_title[:100] + "..."
                    print(f"     ğŸ“ ë¦¬ë·°ì œëª©: {review_title}")

                if metadata.get('review_content'):
                    review_content = metadata['review_content']
                    if len(review_content) > 150:
                        review_content = review_content[:150] + "..."
                    print(f"     ğŸ“„ ë¦¬ë·°ë‚´ìš©: {review_content}")

                # ë¦¬ë·° ê°ì • í‚¤ì›Œë“œ (JSON íŒŒì‹± í•„ìš”)
                if metadata.get('review_emotion_keywords'):
                    try:
                        import json
                        kw = json.loads(metadata['review_emotion_keywords'])
                        if isinstance(kw, list):
                            print(f"     ğŸ’¬ ë¦¬ë·°ê°ì •: {', '.join(kw)}")
                        else:
                            print(f"     ğŸ’¬ ë¦¬ë·°ê°ì •: {kw}")
                    except:
                        print(f"     ğŸ’¬ ë¦¬ë·°ê°ì •: {metadata['review_emotion_keywords'][:50]}...")

                # ì¶”ê°€ ì •ë³´

                if metadata.get('reg_date'):
                    print(f"     ğŸ“… ë“±ë¡ì¼: {metadata['reg_date']}")

    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys

    check_vector_database()

    # ìƒ˜í”Œ ê²€ìƒ‰ë„ ê¸°ë³¸ì ìœ¼ë¡œ ì‹¤í–‰
    if len(sys.argv) > 1 and sys.argv[1] == "search":
        sample_search()
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ ìƒ˜í”Œ ê²€ìƒ‰ë„ ì‹¤í–‰
        sample_search()

if __name__ == "__main__":
    main()