"""
WikiTextProcessor ì§ê´€ì ì¸ TDD í…ŒìŠ¤íŠ¸
ê° í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥ìœ¼ë¡œ ì§„í–‰ìƒí™©ì„ ì‹¤ì‹œê°„ í™•ì¸

ì‹¤í–‰ ë°©ë²•:
    cd ai-service
    python tests/unit/utils/test_wiki_text_processing.py
    ë˜ëŠ”
    python -m pytest tests/unit/utils/test_wiki_text_processing.py -v -s
"""

import pytest
import sys
import os
import json
import time
from pathlib import Path
from unittest.mock import Mock, MagicMock

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# ì˜ì¡´ì„± ëª¨ë“ˆ Mock ì„¤ì •
mock_modules = ['utils', 'models', 'tools', 'chains', 'prompts']
for module_name in mock_modules:
    if module_name not in sys.modules:
        sys.modules[module_name] = Mock()

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ import
from app.utils.wiki_text_processing import WikiTextProcessor


class TestWikiTextProcessorBasics:
    """WikiTextProcessor ê¸°ë³¸ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""

    def test_extract_author_name_with_llm_success(self):
        """LLMì„ ì‚¬ìš©í•œ ì‘ê°€ëª… ì¶”ì¶œ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Mock LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        mock_llm_client = Mock()
        mock_response = Mock()
        mock_choice = Mock()
        mock_message = Mock()

        mock_llm_client.chat.completions.create.return_value = mock_response
        mock_response.choices = [mock_choice]
        mock_choice.message = mock_message
        mock_message.content = json.dumps({
            "author_name": "í•œê°•",
            "confidence": 0.9
        })

        print(f"  ğŸ¤– LLMì„ ì‚¬ìš©í•œ ì‘ê°€ëª… ì¶”ì¶œ...")
        print(f"  ğŸ“ ì…ë ¥ ì¿¼ë¦¬: 'í•œê°•ì´ ëˆ„êµ¬ì•¼'")
        print(f"  ğŸ¯ LLM ì‘ë‹µ: ì‘ê°€ëª…='í•œê°•', ì‹ ë¢°ë„=0.9")

        result = WikiTextProcessor.extract_author_name("í•œê°•ì´ ëˆ„êµ¬ì•¼", mock_llm_client)

        print(f"  ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
        print(f"  âœ… LLM í˜¸ì¶œë¨: {mock_llm_client.chat.completions.create.called}")
        print(f"  âœ… ì‘ê°€ëª… ì •í™•ì„±: {'í•œê°•' == result}")

        assert result == "í•œê°•"
        assert mock_llm_client.chat.completions.create.called
        print("âœ… LLMì„ ì‚¬ìš©í•œ ì‘ê°€ëª… ì¶”ì¶œ ì„±ê³µ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_extract_author_name_with_llm_low_confidence_fallback(self):
        """LLM ë‚®ì€ ì‹ ë¢°ë„ë¡œ í´ë°± í…ŒìŠ¤íŠ¸"""
        mock_llm_client = Mock()
        mock_response = Mock()
        mock_choice = Mock()
        mock_message = Mock()

        mock_llm_client.chat.completions.create.return_value = mock_response
        mock_response.choices = [mock_choice]
        mock_choice.message = mock_message
        mock_message.content = json.dumps({
            "author_name": "í•œê°•",
            "confidence": 0.5  # ë‚®ì€ ì‹ ë¢°ë„
        })

        print(f"  ğŸ“‰ LLM ë‚®ì€ ì‹ ë¢°ë„ í´ë°± í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“ ì…ë ¥: 'í•œê°•ì´ ëˆ„êµ¬ì•¼'")
        print(f"  ğŸ¤– LLM ì‹ ë¢°ë„: 0.5 (ì„ê³„ê°’ 0.7 ë¯¸ë§Œ)")
        print(f"  ğŸ”„ í´ë°± ë°©ì‹ìœ¼ë¡œ ì „í™˜ë¨")

        result = WikiTextProcessor.extract_author_name("í•œê°•ì´ ëˆ„êµ¬ì•¼", mock_llm_client)

        print(f"  ğŸ“Š í´ë°± ì¶”ì¶œ ê²°ê³¼: '{result}'")
        print(f"  âœ… ê²°ê³¼ ì •í™•ì„±: {'í•œê°•' == result}")

        assert result == "í•œê°•"  # í´ë°± ë°©ì‹ìœ¼ë¡œë„ ì •í™•íˆ ì¶”ì¶œ
        print("âœ… LLM ë‚®ì€ ì‹ ë¢°ë„ í´ë°± í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_extract_author_name_llm_failure_fallback(self):
        """LLM ì‹¤íŒ¨ì‹œ í´ë°± í…ŒìŠ¤íŠ¸"""
        mock_llm_client = Mock()
        mock_llm_client.chat.completions.create.side_effect = Exception("API Error")

        print(f"  âš ï¸ LLM ì‹¤íŒ¨ì‹œ í´ë°± í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“ ì…ë ¥: 'í•œê°•ì´ ëˆ„êµ¬ì•¼'")
        print(f"  ğŸš¨ LLM API ì˜¤ë¥˜ ë°œìƒ")
        print(f"  ğŸ”„ í´ë°± ë°©ì‹ìœ¼ë¡œ ìë™ ì „í™˜")

        result = WikiTextProcessor.extract_author_name("í•œê°•ì´ ëˆ„êµ¬ì•¼", mock_llm_client)

        print(f"  ğŸ“Š í´ë°± ì¶”ì¶œ ê²°ê³¼: '{result}'")
        print(f"  âœ… ì˜¤ë¥˜ ì²˜ë¦¬ ì„±ê³µ: LLM ì‹¤íŒ¨ì—ë„ ê²°ê³¼ ë°˜í™˜")
        print(f"  âœ… ê²°ê³¼ ì •í™•ì„±: {'í•œê°•' == result}")

        assert result == "í•œê°•"  # í´ë°±ìœ¼ë¡œ ì •ìƒ ì¶”ì¶œ
        print("âœ… LLM ì‹¤íŒ¨ì‹œ í´ë°± í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_fallback_extract_author_name_who_patterns(self):
        """í´ë°± ë°©ì‹ - 'ëˆ„êµ¬' íŒ¨í„´ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("í•œê°•ê°€ ëˆ„êµ¬", "í•œê°•"),
            ("í•œê°•ì´ ëˆ„êµ¬", "í•œê°•"),
            ("ì´ë§ë…„ì´ ëˆ„êµ¬ì•¼", "ì´ë§ë…„"),
            ("ê¹€ì˜í•˜ ëˆ„êµ¬", "ê¹€ì˜í•˜"),
        ]

        print(f"  ğŸ” 'ëˆ„êµ¬' íŒ¨í„´ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor._fallback_extract_author_name(query)

            print(f"       ğŸ“Š ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… 'ëˆ„êµ¬' íŒ¨í„´ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_fallback_extract_foreign_author_names(self):
        """í´ë°± ë°©ì‹ - ì™¸êµ­ì¸ ì‘ê°€ëª… íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤", "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤"),
            ("ì¡°ì§€ ì˜¤ì›°", "ì¡°ì§€ ì˜¤ì›°"),
            ("ê°€ë¸Œë¦¬ì—˜ ê°€ë¥´ì‹œì•„ ë§ˆë¥´ì¼€ìŠ¤", "ê°€ë¸Œë¦¬ì—˜ ê°€ë¥´ì‹œì•„"),  # ì²« ë‘ ë‹¨ì–´ë§Œ
        ]

        print(f"  ğŸŒ ì™¸êµ­ì¸ ì‘ê°€ëª… íŒ¨í„´ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor._fallback_extract_author_name(query)

            print(f"       ğŸ“Š ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì™¸êµ­ì¸ ì‘ê°€ëª… íŒ¨í„´ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorClarification:
    """WikiTextProcessor ëª…í™•í™” ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

    def test_parse_clarification_response_number_patterns(self):
        """ëª…í™•í™” ì‘ë‹µ íŒŒì‹± - ìˆ«ì ì„ íƒ íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        context = {
            'clarification_candidates': ['í•œê°•', 'ê¹€ì˜í•˜', 'ì´ë§ë…„']
        }

        test_cases = [
            ("1", "í•œê°•"),
            ("2ë²ˆ", "ê¹€ì˜í•˜"),
            ("3ë²ˆì§¸", "ì´ë§ë…„"),
            ("ì²«ë²ˆì§¸", "í•œê°•"),
            ("ë‘ë²ˆì§¸", "ê¹€ì˜í•˜"),
            ("ì„¸ë²ˆì§¸", "ì´ë§ë…„"),
        ]

        print(f"  ğŸ”¢ ìˆ«ì ì„ íƒ íŒ¨í„´ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í›„ë³´: {context['clarification_candidates']}")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì…ë ¥: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.parse_clarification_response(query, context)

            print(f"       ğŸ“Š ì„ íƒ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ìˆ«ì ì„ íƒ íŒ¨í„´ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_parse_clarification_response_direct_mention(self):
        """ëª…í™•í™” ì‘ë‹µ íŒŒì‹± - ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        context = {}

        test_cases = [
            ("í•œê°• ë§í•˜ëŠ”ê±°ì•¼", "í•œê°•"),
            ("ê¹€ì˜í•˜ ë§ì•„", "ê¹€ì˜í•˜"),
            ("ì´ë§ë…„ì´ì•¼", "ì´ë§ë…„"),
            ("ë°•ë¯¼ê·œìš”", "ë°•ë¯¼ê·œ"),
        ]

        print(f"  ğŸ’¬ ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì…ë ¥: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.parse_clarification_response(query, context)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_parse_clarification_response_invalid_cases(self):
        """ëª…í™•í™” ì‘ë‹µ íŒŒì‹± - ìœ íš¨í•˜ì§€ ì•Šì€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        context = {
            'clarification_candidates': ['í•œê°•', 'ê¹€ì˜í•˜']
        }

        test_cases = [
            ("10", None),  # ë²”ìœ„ ì´ˆê³¼
            ("ì•Œ ìˆ˜ ì—†ìŒ", None),  # íŒ¨í„´ ë¶ˆì¼ì¹˜
            ("", None),  # ë¹ˆ ë¬¸ìì—´
            ("ë­”ê°€ ì´ìƒí•´", None),  # ì¸ì‹ ë¶ˆê°€
        ]

        print(f"  âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì…ë ¥: '{query}' â†’ ì˜ˆìƒ: {expected}")

            result = WikiTextProcessor.parse_clarification_response(query, context)

            print(f"       ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {result}")
            print(f"       âœ… ì˜ˆìƒëŒ€ë¡œ None ë°˜í™˜: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ìœ íš¨í•˜ì§€ ì•Šì€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorContextQuestions:
    """WikiTextProcessor ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

    def test_extract_author_from_context_question_success(self):
        """ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ì—ì„œ ì‘ê°€ëª… ì¶”ì¶œ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ì´ë§ë…„ì€ ì–´ë”” ëŒ€í•™", "ì´ë§ë…„"),
            ("í•œê°•ì´ ì–¸ì œ", "í•œê°•"),
            ("ê¹€ì˜í•˜ ëŒ€í•™", "ê¹€ì˜í•˜"),
            ("ë°•ë¯¼ê·œ ì¶œìƒ", "ë°•ë¯¼ê·œ"),
            ("ì¡°ì •ë˜ëŠ” ëª‡", "ì¡°ì •ë˜"),
        ]

        print(f"  ğŸ” ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
        print(f"  ğŸ¯ ëª©í‘œ: 'ì‘ê°€ëª…ì€/ì´ ì§ˆë¬¸ì–´' íŒ¨í„´ì—ì„œ ì‘ê°€ëª… ì¶”ì¶œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.extract_author_from_context_question(query)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_extract_author_from_context_question_invalid(self):
        """ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ - ìœ íš¨í•˜ì§€ ì•Šì€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ì–´ë”” ëŒ€í•™", None),  # ì‘ê°€ëª… ì—†ìŒ
            ("ë­”ê°€ ì´ìƒí•´", None),  # íŒ¨í„´ ë¶ˆì¼ì¹˜
            ("", None),  # ë¹ˆ ë¬¸ìì—´
            ("ê·¸ëƒ¥ ê¶ê¸ˆí•´", None),  # ì‘ê°€ëª… íŒ¨í„´ ì—†ìŒ
        ]

        print(f"  âŒ ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ë¬´íš¨ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}' â†’ ì˜ˆìƒ: {expected}")

            result = WikiTextProcessor.extract_author_from_context_question(query)

            print(f"       ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {result}")
            print(f"       âœ… ì˜ˆìƒëŒ€ë¡œ None ë°˜í™˜: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ë¬´íš¨ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorBookExtraction:
    """WikiTextProcessor ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""

    def test_extract_book_title_from_query_basic(self):
        """ì¿¼ë¦¬ì—ì„œ ê¸°ë³¸ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ê°œë¯¸ ì‘ê°€ ëˆ„êµ¬ì•¼", "ê°œë¯¸"),
            ("ì±„ì‹ì£¼ì˜ìë¥¼ ì“´ ì‘ê°€", "ì±„ì‹ì£¼ì˜ì"),
            ("í† ì§€ëŠ” ëˆ„ê°€ ì¼ì–´", "í† ì§€"),
            ("í•´ë¦¬í¬í„° ì €ì ì•Œë ¤ì¤˜", "í•´ë¦¬í¬í„°"),
            ("1984 ì“´ì´ê°€ ëˆ„êµ¬ì•¼?", "1984"),
        ]

        print(f"  ğŸ“š ê¸°ë³¸ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
        print(f"  ğŸ¯ ëª©í‘œ: ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì œê±° í›„ ì±… ì œëª©ë§Œ ì¶”ì¶œ")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.extract_book_title_from_query(query)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ê¸°ë³¸ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_extract_book_title_complex_cases(self):
        """ë³µì¡í•œ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ì˜¤ë§Œê³¼ í¸ê²¬ ì‘ê°€", "ì˜¤ë§Œ í¸ê²¬"),  # 'ê³¼' ì œê±°
            ("ê·¸ë¦¬ê³  ì•„ë¬´ ë§ í•˜ì§€ ì•Šì•˜ë‹¤ ì €ì", "ì™€ ì•„ë¬´ ë§ í•˜ì§€ ì•Šì•˜ë‹¤"),  # 'ê·¸ë¦¬ê³ ' â†’ 'ì™€'
            ("í•´ë¦¬í¬í„°ì™€ ë§ˆë²•ì‚¬ì˜ ëŒ ëˆ„ê°€ ì¼ì–´", "í•´ë¦¬í¬í„° ë§ˆë²•ì‚¬ ëŒ"),
        ]

        print(f"  ğŸ”§ ë³µì¡í•œ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
        print(f"  ğŸ¯ ëª©í‘œ: ì ‘ì†ì‚¬ ë³€í™˜ ë° ë³µì¡í•œ í‚¤ì›Œë“œ ì²˜ë¦¬")

        for i, (query, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}'")
            print(f"       ğŸ”„ ì²˜ë¦¬: ì ‘ì†ì‚¬ ë³€í™˜, í‚¤ì›Œë“œ ì œê±°")
            print(f"       ğŸ¯ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.extract_book_title_from_query(query)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ë³µì¡í•œ ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_handle_conjunction_in_title(self):
        """ì œëª©ì˜ ì ‘ì†ì‚¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            ("ì˜¤ë§Œ ê·¸ë¦¬ê³  í¸ê²¬", "ì˜¤ë§Œ ì™€ í¸ê²¬"),
            ("í•´ë¦¬í¬í„° í•˜ê³  ë§ˆë²•ì‚¬", "í•´ë¦¬í¬í„° ì™€ ë§ˆë²•ì‚¬"),
            ("ëˆ ë‘ ê¶Œë ¥", "ëˆ ì™€ ê¶Œë ¥"),
            ("ì‚¬ë‘ ì´ë‘ ì „ìŸ", "ì‚¬ë‘ ì™€ ì „ìŸ"),
        ]

        print(f"  ğŸ”— ì ‘ì†ì‚¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
        print(f"  ğŸ¯ ëª©í‘œ: ë‹¤ì–‘í•œ ì ‘ì†ì‚¬ë¥¼ 'ì™€'ë¡œ í†µì¼")

        for i, (title, expected) in enumerate(test_cases, 1):
            print(f"    {i}. ì…ë ¥: '{title}' â†’ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor._handle_conjunction_in_title(title)

            print(f"       ğŸ“Š ë³€í™˜ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì ‘ì†ì‚¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorIntegration:
    """WikiTextProcessor í†µí•© í…ŒìŠ¤íŠ¸"""

    def test_full_author_extraction_workflow(self):
        """ì „ì²´ ì‘ê°€ëª… ì¶”ì¶œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        test_scenarios = [
            {
                "query": "í•œê°•ì´ ëˆ„êµ¬ì•¼",
                "expected_author": "í•œê°•",
                "description": "ë‹¨ìˆœ ì‘ê°€ëª… ì§ˆë¬¸",
                "method": "direct_question"
            },
            {
                "query": "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤ ì‘í’ˆ ì•Œë ¤ì¤˜",
                "expected_author": "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤",
                "description": "ì™¸êµ­ ì‘ê°€ëª… + ìš”ì²­",
                "method": "foreign_name"
            },
            {
                "query": "ê°œë¯¸ ì“´ ì‘ê°€",
                "expected_book": "ê°œë¯¸",
                "description": "ì±… ì œëª©ìœ¼ë¡œ ì‘ê°€ ì—­ì¶”ì ",
                "method": "book_reverse"
            }
        ]

        print(f"  ğŸ”„ ì „ì²´ ì‘ê°€ëª… ì¶”ì¶œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤: {len(test_scenarios)}ê°œ")

        for i, scenario in enumerate(test_scenarios, 1):
            print(f"    {i}. {scenario['description']}")
            print(f"       ğŸ“ ì¿¼ë¦¬: '{scenario['query']}'")
            print(f"       ğŸ¯ ë°©ë²•: {scenario['method']}")

            if "expected_author" in scenario:
                result = WikiTextProcessor.extract_author_name(scenario["query"])
                expected = scenario["expected_author"]

                print(f"       ğŸ“Š ì‘ê°€ëª… ì¶”ì¶œ: '{result}'")
                print(f"       âœ… ì •í™•ì„±: {expected == result}")
                assert result == expected

            if "expected_book" in scenario:
                result = WikiTextProcessor.extract_book_title_from_query(scenario["query"])
                expected = scenario["expected_book"]

                print(f"       ğŸ“Š ì±… ì œëª© ì¶”ì¶œ: '{result}'")
                print(f"       âœ… ì •í™•ì„±: {expected == result}")
                assert result == expected

            print(f"       âœ… ì‹œë‚˜ë¦¬ì˜¤ {i} í†µê³¼")

        print("âœ… ì „ì²´ ì‘ê°€ëª… ì¶”ì¶œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_clarification_workflow_integration(self):
        """ëª…í™•í™” ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
        print(f"  ğŸ” ëª…í™•í™” ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸...")

        # 1ë‹¨ê³„: ë™ëª…ì´ì¸ ìƒí™© ì‹œë®¬ë ˆì´ì…˜
        context = {
            'clarification_candidates': ['í•œê°•(ì†Œì„¤ê°€)', 'í•œê°•(ê°•)', 'í•œê°•(ì§€ëª…)']
        }

        print(f"    1ï¸âƒ£ ë™ëª…ì´ì¸ ìƒí™© ì„¤ì •")
        print(f"       ğŸ“‹ í›„ë³´: {context['clarification_candidates']}")

        # 2ë‹¨ê³„: ì‚¬ìš©ì ì„ íƒ íŒŒì‹±
        user_responses = [
            ("1", "í•œê°•(ì†Œì„¤ê°€)"),
            ("ì²«ë²ˆì§¸", "í•œê°•(ì†Œì„¤ê°€)"),
            ("í•œê°• ì†Œì„¤ê°€ ë§í•˜ëŠ”ê±°ì•¼", "í•œê°•"),
        ]

        print(f"    2ï¸âƒ£ ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")

        for j, (response, expected_contain) in enumerate(user_responses, 1):
            print(f"       {j}. ì‘ë‹µ: '{response}'")

            result = WikiTextProcessor.parse_clarification_response(response, context)

            print(f"          ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: '{result}'")

            if "í•œê°•" in expected_contain:  # ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                assert result and "í•œê°•" in result
                print(f"          âœ… 'í•œê°•' í¬í•¨ í™•ì¸")
            else:
                assert result == expected_contain
                print(f"          âœ… ì •í™•í•œ ë§¤ì¹­ í™•ì¸")

            print(f"          âœ… ì‘ë‹µ {j} ì²˜ë¦¬ ì™„ë£Œ")

        print("âœ… ëª…í™•í™” ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorPerformance:
    """WikiTextProcessor ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    def test_extraction_performance_large_batch(self):
        """ëŒ€ëŸ‰ ì¶”ì¶œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        queries = [
                      "í•œê°•ì´ ëˆ„êµ¬ì•¼",
                      "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤",
                      "ê¹€ì˜í•˜ ì‘ê°€",
                      "ê°œë¯¸ ì €ì",
                      "ì±„ì‹ì£¼ì˜ì ì“´ ì‚¬ëŒ"
                  ] * 100  # 500ê°œ ì¿¼ë¦¬

        print(f"  âš¡ ëŒ€ëŸ‰ ì¶”ì¶œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        print(f"    - í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜: {len(queries):,}ê°œ")
        print(f"    - ëª©í‘œ ì‹œê°„: 5ì´ˆ ì´ë‚´")

        start_time = time.time()

        results = []
        for i, query in enumerate(queries):
            result = WikiTextProcessor.extract_author_name(query)
            results.append(result)

            if (i + 1) % 100 == 0:
                elapsed = time.time() - start_time
                print(f"      ğŸ’¾ ì§„í–‰: {i + 1:,}ê°œ ì²˜ë¦¬ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")

        end_time = time.time()
        processing_time = end_time - start_time

        print(f"    ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        print(f"      - ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.4f}ì´ˆ")
        print(f"      - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {processing_time/len(queries):.6f}ì´ˆ/ê°œ")
        print(f"      - ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {len(queries)/processing_time:.1f}ê°œ/ì´ˆ")
        print(f"      - ê²°ê³¼ ìˆ˜: {len(results):,}ê°œ")

        # ì„±ëŠ¥ ê²€ì¦ (500ê°œ ì¿¼ë¦¬ë¥¼ 5ì´ˆ ì´ë‚´ ì²˜ë¦¬)
        assert processing_time < 5.0
        assert len(results) == len(queries)
        assert all(isinstance(r, str) for r in results)

        print("âœ… ëŒ€ëŸ‰ ì¶”ì¶œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_memory_usage_stability(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        try:
            import psutil
        except ImportError:
            print("  âš ï¸ psutil ëª¨ë“ˆì´ ì—†ì–´ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸...")

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        print(f"    - ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory / 1024 / 1024:.2f}MB")

        # ëŒ€ëŸ‰ ì²˜ë¦¬
        for i in range(1000):
            WikiTextProcessor.extract_author_name("í•œê°•ì´ ëˆ„êµ¬ì•¼")
            WikiTextProcessor.extract_book_title_from_query("ê°œë¯¸ ì‘ê°€ ëˆ„êµ¬ì•¼")

            if (i + 1) % 200 == 0:
                current_memory = process.memory_info().rss
                print(f"      ğŸ’½ {i + 1:,}íšŒ ì²˜ë¦¬ í›„: {current_memory / 1024 / 1024:.2f}MB")

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        print(f"    ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²°ê³¼:")
        print(f"      - ìµœì¢… ë©”ëª¨ë¦¬: {final_memory / 1024 / 1024:.2f}MB")
        print(f"      - ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase / 1024 / 1024:.2f}MB")
        print(f"      - ì¦ê°€ìœ¨: {(memory_increase / initial_memory) * 100:.2f}%")

        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ 50MB ì´í•˜ì¸ì§€ í™•ì¸
        assert memory_increase < 50 * 1024 * 1024

        print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorErrorHandling:
    """WikiTextProcessor ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

    def test_handle_invalid_input_types(self):
        """ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ íƒ€ì… ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        invalid_inputs = [None, 123, [], {}, True, 3.14]

        print(f"  ğŸš« ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ íƒ€ì… ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì…ë ¥: {len(invalid_inputs)}ê°œ")

        for i, test_input in enumerate(invalid_inputs, 1):
            print(f"    {i}. ì…ë ¥ íƒ€ì…: {type(test_input).__name__} = {test_input}")

            try:
                result = WikiTextProcessor.extract_author_name(test_input)
                print(f"       âš ï¸ ì˜ˆì™¸ê°€ ë°œìƒí•´ì•¼ í•˜ëŠ”ë° ê²°ê³¼ ë°˜í™˜: '{result}'")
                assert False, f"ì˜ˆì™¸ê°€ ë°œìƒí•´ì•¼ í•¨: {test_input}"

            except AttributeError as e:
                print(f"       âœ… ì˜ˆìƒëœ AttributeError ë°œìƒ: {e}")
                assert True

            except Exception as e:
                print(f"       âœ… ì ì ˆí•œ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                assert isinstance(e, (TypeError, ValueError, AttributeError))

            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ íƒ€ì… ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_handle_empty_and_whitespace_input(self):
        """ë¹ˆ ë¬¸ìì—´ ë° ê³µë°± ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        empty_inputs = ["", "   ", "\n\n", "\t\t", "    \n    "]

        print(f"  ğŸ—³ï¸ ë¹ˆ ë¬¸ìì—´ ë° ê³µë°± ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì…ë ¥: {len(empty_inputs)}ê°œ")

        for i, test_input in enumerate(empty_inputs, 1):
            print(f"    {i}. ì…ë ¥: {repr(test_input)}")

            result = WikiTextProcessor.extract_author_name(test_input)
            book_result = WikiTextProcessor.extract_book_title_from_query(test_input)

            print(f"       ğŸ“Š ì‘ê°€ëª… ì¶”ì¶œ: '{result}'")
            print(f"       ğŸ“Š ì±… ì œëª© ì¶”ì¶œ: '{book_result}'")
            print(f"       âœ… ë¹ˆ ê²°ê³¼ ë°˜í™˜: {result == ''}")

            # ë¹ˆ ë¬¸ìì—´ì´ ë°˜í™˜ë˜ì–´ì•¼ í•¨
            assert result == ""
            assert book_result == "" or book_result.strip() == ""

            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ë¹ˆ ë¬¸ìì—´ ë° ê³µë°± ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_handle_empty_context_gracefully(self):
        """ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ìš°ì•„í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        empty_contexts = [{}, None, {'other_key': 'value'}]

        print(f"  ğŸ” ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ìš°ì•„í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸: {len(empty_contexts)}ê°œ")

        for i, context in enumerate(empty_contexts, 1):
            print(f"    {i}. ì»¨í…ìŠ¤íŠ¸: {context}")

            result = WikiTextProcessor.parse_clarification_response("1", context)

            print(f"       ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {result}")
            print(f"       âœ… None ë°˜í™˜: {result is None}")

            assert result is None
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ìš°ì•„í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_handle_malformed_llm_response(self):
        """ì˜ëª»ëœ LLM ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        malformed_responses = [
            "Invalid JSON",
            '{"incomplete": "json"',
            '{"author_name": null}',
            '{"confidence": "not_a_number"}',
            "",
            "Just plain text"
        ]

        print(f"  ğŸ¤– ì˜ëª»ëœ LLM ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {len(malformed_responses)}ê°œ")

        for i, malformed_response in enumerate(malformed_responses, 1):
            print(f"    {i}. LLM ì‘ë‹µ: '{malformed_response}'")

            mock_llm_client = Mock()
            mock_response = Mock()
            mock_choice = Mock()
            mock_message = Mock()

            mock_llm_client.chat.completions.create.return_value = mock_response
            mock_response.choices = [mock_choice]
            mock_choice.message = mock_message
            mock_message.content = malformed_response

            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í´ë°± ë™ì‘ í™•ì¸
            result = WikiTextProcessor.extract_author_name("í•œê°•ì´ ëˆ„êµ¬ì•¼", mock_llm_client)

            print(f"       ğŸ“Š í´ë°± ê²°ê³¼: '{result}'")
            print(f"       âœ… í´ë°± ì„±ê³µ: {'í•œê°•' == result}")

            # í´ë°±ìœ¼ë¡œ ì •ìƒ ì¶”ì¶œë˜ì–´ì•¼ í•¨
            assert result == "í•œê°•"
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ì˜ëª»ëœ LLM ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestWikiTextProcessorEdgeCases:
    """WikiTextProcessor ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""

    def test_unicode_and_special_characters(self):
        """ìœ ë‹ˆì½”ë“œ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        unicode_test_cases = [
            ("ê¹€ì˜í•˜ğŸ­ ëˆ„êµ¬ì•¼", "ê¹€ì˜í•˜"),  # ì´ëª¨ì§€ í¬í•¨
            ("JosÃ© SaramagoëŠ” ëˆ„êµ¬", "JosÃ© Saramago"),  # ë¼í‹´ ë¬¸ì
            ("ĞœÑƒÑ€Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ¥Ğ°Ñ€ÑƒĞºĞ¸", "ĞœÑƒÑ€Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ¥Ğ°Ñ€ÑƒĞºĞ¸"),  # í‚¤ë¦´ ë¬¸ì
            ("æ‘ä¸Šæ˜¥æ¨¹ ì‘ê°€", "æ‘ä¸Šæ˜¥æ¨¹"),  # í•œì
            ("Î±Â·Î²Â·Î³ ê·¸ë¦¬ìŠ¤ë¬¸ì í¬í•¨í•œ ê¹€ì˜í•˜", "ê¹€ì˜í•˜"),  # ê·¸ë¦¬ìŠ¤ ë¬¸ì
        ]

        print(f"  ğŸŒ ìœ ë‹ˆì½”ë“œ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(unicode_test_cases)}ê°œ")

        for i, (query, expected) in enumerate(unicode_test_cases, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}'")
            print(f"       ğŸ¯ ì˜ˆìƒ: '{expected}'")

            try:
                result = WikiTextProcessor.extract_author_name(query)

                print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
                print(f"       âœ… ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬ ì„±ê³µ")

                # ê²°ê³¼ê°€ ë¬¸ìì—´ì´ê³  ë¹„ì–´ìˆì§€ ì•Šì•„ì•¼ í•¨
                assert isinstance(result, str)

                # ì˜ˆìƒ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ê±°ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ í¬í•¨í•´ì•¼ í•¨
                if expected in result or result in expected:
                    print(f"       âœ… ë‚´ìš© ì¼ì¹˜ ë˜ëŠ” ë¶€ë¶„ ì¼ì¹˜")
                else:
                    print(f"       âš ï¸ ë¶ˆì¼ì¹˜í•˜ì§€ë§Œ ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬ëŠ” ì„±ê³µ")

                print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

            except Exception as e:
                print(f"       âŒ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                # ìœ ë‹ˆì½”ë“œ ê´€ë ¨ ì˜ˆì™¸ë§Œ í—ˆìš©
                assert isinstance(e, (UnicodeError, UnicodeDecodeError, UnicodeEncodeError))
                print(f"       âœ… ì ì ˆí•œ ìœ ë‹ˆì½”ë“œ ì˜ˆì™¸ ì²˜ë¦¬")

        print("âœ… ìœ ë‹ˆì½”ë“œ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_very_long_text_handling(self):
        """ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ìƒì„±
        base_text = "ê¹€ì˜í•˜ëŠ” 1968ë…„ì— íƒœì–´ë‚œ í•œêµ­ì˜ ì†Œì„¤ê°€ì´ë‹¤. "
        long_text = base_text * 1000  # ì•½ 44,000ì
        long_query = long_text + "ê¹€ì˜í•˜ê°€ ëˆ„êµ¬ì•¼"

        print(f"  ğŸ“ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"    - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(long_query):,}ì")
        print(f"    - ëª©í‘œ: 5ì´ˆ ì´ë‚´ ì²˜ë¦¬")

        start_time = time.time()

        result = WikiTextProcessor.extract_author_name(long_query)

        end_time = time.time()
        processing_time = end_time - start_time

        print(f"    ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"      - ì²˜ë¦¬ ì‹œê°„: {processing_time:.4f}ì´ˆ")
        print(f"      - ì¶”ì¶œ ê²°ê³¼: '{result}'")
        print(f"      - ê²°ê³¼ ì •í™•ì„±: {'ê¹€ì˜í•˜' == result}")

        # ì„±ëŠ¥ ë° ì •í™•ì„± ê²€ì¦
        assert processing_time < 5.0
        assert result == "ê¹€ì˜í•˜"

        print("âœ… ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_mixed_language_queries(self):
        """ë‹¤êµ­ì–´ í˜¼í•© ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        mixed_queries = [
            ("Murakami Haruki ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤ ëˆ„êµ¬ì•¼", "Murakami Haruki"),
            ("ê¹€ì˜í•˜ Kim Young-ha author", "ê¹€ì˜í•˜"),
            ("í•œê°• Han Kang writer information", "í•œê°•"),
            ("George Orwell ì¡°ì§€ ì˜¤ì›° ì‘ê°€", "George Orwell"),
        ]

        print(f"  ğŸŒ ë‹¤êµ­ì–´ í˜¼í•© ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(mixed_queries)}ê°œ")

        for i, (query, expected_contain) in enumerate(mixed_queries, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}'")
            print(f"       ğŸ¯ ì˜ˆìƒ í¬í•¨: '{expected_contain}'")

            result = WikiTextProcessor.extract_author_name(query)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")

            # ì˜ˆìƒ ë¬¸ìì—´ì´ ê²°ê³¼ì— í¬í•¨ë˜ì–´ì•¼ í•¨ (ìˆœì„œë‚˜ í˜•íƒœ ë¬´ê´€)
            contains_expected = expected_contain in result or any(
                word in result for word in expected_contain.split()
            )

            print(f"       âœ… ì˜ˆìƒ ë‚´ìš© í¬í•¨: {contains_expected}")

            assert contains_expected or result != ""  # ìµœì†Œí•œ ë¹ˆ ê²°ê³¼ê°€ ì•„ë‹ˆì–´ì•¼ í•¨
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ë‹¤êµ­ì–´ í˜¼í•© ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_complex_punctuation_handling(self):
        """ë³µì¡í•œ êµ¬ë‘ì  ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        punctuation_queries = [
            ("ê¹€ì˜í•˜ê°€... ëˆ„êµ¬ì•¼???", "ê¹€ì˜í•˜"),
            ("í•œê°•ì´!!! ì •ë§ ëˆ„êµ¬ì¸ì§€ ì•Œë ¤ì¤˜!!!", "í•œê°•"),
            ("\"ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤\"ëŠ” ëˆ„êµ¬ì…ë‹ˆê¹Œ?", "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤"),
            ("(ì´ë§ë…„) ì‘ê°€ì— ëŒ€í•´...", "ì´ë§ë…„"),
            ("ê¹€ì˜í•˜, í•œê°•... ì´ ì‘ê°€ë“¤ì€?", "ê¹€ì˜í•˜"),  # ì²« ë²ˆì§¸ë§Œ ì¶”ì¶œ
        ]

        print(f"  ğŸ“ ë³µì¡í•œ êµ¬ë‘ì  ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(punctuation_queries)}ê°œ")

        for i, (query, expected) in enumerate(punctuation_queries, 1):
            print(f"    {i}. ì¿¼ë¦¬: '{query}'")
            print(f"       ğŸ¯ ì˜ˆìƒ: '{expected}'")

            result = WikiTextProcessor.extract_author_name(query)

            print(f"       ğŸ“Š ì¶”ì¶œ ê²°ê³¼: '{result}'")
            print(f"       âœ… ì •í™•ì„±: {expected == result}")

            assert result == expected
            print(f"       âœ… í…ŒìŠ¤íŠ¸ {i} í†µê³¼")

        print("âœ… ë³µì¡í•œ êµ¬ë‘ì  ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")


if __name__ == "__main__":
    start_time = time.time()

    print("ğŸ§ª WikiTextProcessor ì§ê´€ì ì¸ TDD í…ŒìŠ¤íŠ¸ ì‹œì‘\n")

    # ê¸°ë³¸ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    print("ğŸ“‹ ê¸°ë³¸ ì‘ê°€ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸ - LLM ë° í´ë°± ë°©ì‹")
    print("=" * 60)
    test_basics = TestWikiTextProcessorBasics()
    test_basics.test_extract_author_name_with_llm_success()
    print()
    test_basics.test_extract_author_name_with_llm_low_confidence_fallback()
    print()
    test_basics.test_extract_author_name_llm_failure_fallback()
    print()
    test_basics.test_fallback_extract_author_name_who_patterns()
    print()
    test_basics.test_fallback_extract_foreign_author_names()

    # ëª…í™•í™” ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ëª…í™•í™” ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ - ë™ëª…ì´ì¸ í•´ê²°")
    print("=" * 60)
    test_clarification = TestWikiTextProcessorClarification()
    test_clarification.test_parse_clarification_response_number_patterns()
    print()
    test_clarification.test_parse_clarification_response_direct_mention()
    print()
    test_clarification.test_parse_clarification_response_invalid_cases()

    # ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\nğŸ’¬ ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ - ëŒ€í™” ë§¥ë½ ì´í•´")
    print("=" * 60)
    test_context = TestWikiTextProcessorContextQuestions()
    test_context.test_extract_author_from_context_question_success()
    print()
    test_context.test_extract_author_from_context_question_invalid()

    # ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    print("\nğŸ“š ì±… ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸ - ì—­ì¶”ì  ê²€ìƒ‰")
    print("=" * 60)
    test_book = TestWikiTextProcessorBookExtraction()
    test_book.test_extract_book_title_from_query_basic()
    print()
    test_book.test_extract_book_title_complex_cases()
    print()
    test_book.test_handle_conjunction_in_title()

    # í†µí•© í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ í†µí•© í…ŒìŠ¤íŠ¸ - ì „ì²´ ì›Œí¬í”Œë¡œìš° ê²€ì¦")
    print("=" * 60)
    test_integration = TestWikiTextProcessorIntegration()
    test_integration.test_full_author_extraction_workflow()
    print()
    test_integration.test_clarification_workflow_integration()

    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - ì²˜ë¦¬ ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±")
    print("=" * 60)
    test_performance = TestWikiTextProcessorPerformance()
    test_performance.test_extraction_performance_large_batch()
    print()
    test_performance.test_memory_usage_stability()

    # ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\nğŸš« ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ - ì˜ˆì™¸ ìƒí™© ëŒ€ì‘")
    print("=" * 60)
    test_error = TestWikiTextProcessorErrorHandling()
    test_error.test_handle_invalid_input_types()
    print()
    test_error.test_handle_empty_and_whitespace_input()
    print()
    test_error.test_handle_empty_context_gracefully()
    print()
    test_error.test_handle_malformed_llm_response()

    # ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    print("\nğŸŒŸ ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ - íŠ¹ìˆ˜ ìƒí™© ì²˜ë¦¬")
    print("=" * 60)
    test_edge = TestWikiTextProcessorEdgeCases()
    test_edge.test_unicode_and_special_characters()
    print()
    test_edge.test_very_long_text_handling()
    print()
    test_edge.test_mixed_language_queries()
    print()
    test_edge.test_complex_punctuation_handling()

    end_time = time.time()
    total_time = end_time - start_time

    print("\n" + "=" * 60)
    print("ğŸ‰ ëª¨ë“  WikiTextProcessor í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:")
    print("  âœ… ê¸°ë³¸ ì‘ê°€ëª… ì¶”ì¶œ: 5ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ëª…í™•í™” ì‘ë‹µ ì²˜ë¦¬: 3ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬: 2ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ì±… ì œëª© ì¶”ì¶œ: 3ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… í†µí•© í…ŒìŠ¤íŠ¸: 2ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: 2ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ì˜¤ë¥˜ ì²˜ë¦¬: 4ê°œ í…ŒìŠ¤íŠ¸")
    print("  âœ… ì—£ì§€ ì¼€ì´ìŠ¤: 4ê°œ í…ŒìŠ¤íŠ¸")
    print(f"\nğŸ“ˆ ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: 25ê°œ")
    print("\nğŸ“ pytestë¡œ ì‹¤í–‰í•˜ë ¤ë©´:")
    print("    cd ai-service")
    print("    python -m pytest tests/unit/utils/test_wiki_text_processing.py -v -s")
    print("\nğŸš€ ê°œë³„ ì‹¤í–‰:")
    print("    python tests/unit/utils/test_wiki_text_processing.py")