from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from dotenv import load_dotenv
import os
from config.llm import embedding_model, recommendation_llm

# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
# embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
# recommendation_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# ë²¡í„° DB ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # app/
CHROMA_DIR = os.path.join(BASE_DIR, '..', 'data', 'chroma_db')
vectorstore = Chroma(
    collection_name="bookstore_collection",
    embedding_function=embedding_model,
    persist_directory=os.path.abspath(CHROMA_DIR)
)

# ì¶”ì²œ í”„ë¡¬í”„íŠ¸
recommend_prompt = PromptTemplate(
    input_variables=["emotion", "genre", "keywords", "retrieved_docs"],
    template="""
    ë‹¹ì‹ ì€ ê°ì • ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.

    ì‚¬ìš©ì ìš”ì²­ ì •ë³´:
    - ê°ì •(emotion): {emotion}
    - ì¥ë¥´(genre): {genre}
    - í‚¤ì›Œë“œ: {keywords}

    ì•„ë˜ëŠ” ìœ ì‚¬í•œ ë„ì„œ ì„¤ëª…ë“¤ì…ë‹ˆë‹¤:
    {retrieved_docs}

    {emotion}ì´ ì£¼ì–´ì§€ì§€ ì•Šì•˜ë‹¤ë©´, ì¥ë¥´ì— ì–´ìš¸ë¦¬ëŠ” ì±…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê°ì •ì´ ì—†ë”ë¼ë„ ì¥ë¥´ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì´ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€ 3ê¶Œì˜ ë„ì„œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ê° ì±…ì— ëŒ€í•´ ì œëª©ê³¼ ê°„ë‹¨í•œ ì¶”ì²œ ì´ìœ ë¥¼ ì§§ê²Œ í¬í•¨í•˜ì„¸ìš”.
    ê°™ì€ ì±… ì œëª©ì€ í•œ ë²ˆë§Œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    """
)

# ì¶”ì²œ ì‹¤í–‰ í•¨ìˆ˜
def run_recommend_agent(query_data: dict) -> str:
    emotion = query_data.get("emotion", "")
    genre = query_data.get("genre", "")
    keywords = query_data.get("keywords", [])
    keyword_str = ", ".join(keywords) if isinstance(keywords, list) else str(keywords)

    # ê²€ìƒ‰ ë¬¸ì¥ êµ¬ì„±
    search_query = f"{emotion} {genre} {keyword_str}".strip()
    #docs: list[Document] = vectorstore.similarity_search(search_query, k=3)

    # MMR ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©
    retirever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3, "fetch_k":10, "lambda_mult": 0.7}
    )
    docs= retirever.invoke(search_query)

    if not docs:
        return "âŒ ê´€ë ¨ ë„ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”."

    # ë²¡í„° DBì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš© ë¬¸ìì—´ë¡œ ë³€í™˜
    retrieved_docs = "\n\n".join([f"{i+1}. {doc.page_content}" for i, doc in enumerate(docs)])

    # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
    filled_prompt = recommend_prompt.format(
        emotion=emotion,
        genre=genre,
        keywords=keyword_str,
        retrieved_docs=retrieved_docs
    )

    # LLM í˜¸ì¶œ
    response = recommendation_llm.invoke(filled_prompt)

    return response.content


# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    sample_query = {
        "emotion": "ìš°ìš¸",
        "genre": "ì—ì„¸ì´",
        "keywords": ["ìœ„ë¡œ", "ì‚¶", "í¬ë§"]
    }
    result = run_recommend_agent(sample_query)
    print("ğŸ“š ì¶”ì²œ ê²°ê³¼:\n", result)
